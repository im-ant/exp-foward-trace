# ============================================================================
# Config file for running the SF return experiments
# NOTE: only works for linear prediction currently? 
# ============================================================================

[Training]
num_episodes = 100

# cs_seed = 2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60
cs_seed = 2


[Env]
# Class name string, used to grab the class (if imported)
# cls_string = BoyansChainEnv
# cls_string = RandomWalkChainEnv
#cls_string = RandomWalkChainEnv, BoyansChainEnv
cls_string = PerfBinaryTreeEnv

[Agent]
# Class name string, used to grab the class (if imported)
#cls_string = SFReturnAgent
#cls_string = SarsaLambdaAgent
#cls_string = ExpectedTraceAgent
cls_string = SarsaLambdaAgent, SFReturnAgent

# MDP setting
cs_gamma = 1.0

# Agent learning settings
# cs_lamb = 0.0, 0.1, 0.2, 0.3, 0.5, 0.8, 0.9, 0.95, 1.0
cs_lamb = 0.8
#cs_eta_trace = 0.0, 0.6, 0.9
cs_eta_trace = 0.6
# cs_lr = 0.02, 0.06, 0.1, 0.2
cs_lr = 0.06

# Give the agent the best-fit reward function parameters
use_true_R_fn = False


