#!/bin/bash

# ============================================================================
# Script submitted to sbatch to run a hydra parameter sweep on the cluster
#
# Author: Anthony G. Chen
# ============================================================================

# Format for where to put the parameter sweep outputs
sweep_parent_dir='/network/tmp1/chenant/ant/sr_trace/${now:%Y-%m-%d}/${now:%H-%M-%S}'

# ===========================
# Experimental set-up

# (1.1) Load packages
module load python/3.7
module load python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.0

# (1.2) Load environment
source $HOME/venvs/rlpyt/bin/activate

# (2) no data to copy

# (3) launch job with parameter sweeps
python -u train_rlpyt.py --multirun \
    hydra.run.dir=$sweep_parent_dir \
    hydra.sweep.dir=$sweep_parent_dir \
    runner.kwargs.n_steps=1e5 \
    runner.kwargs.log_interval_steps=5e2 \
    sampler.kwargs.batch_T=2,12,64 \
    training.seed=2,4 \
    environment.kwargs.name='MiniGrid-Empty-5x5-v0' \
    model.kwargs.sf_hidden_sizes=null,[512] \
    algo.kwargs.discount=0.95 \
    algo.kwargs.sf_lambda=0.0,0.7 \
    algo.kwargs.value_loss_coeff=0.5 \
    algo.kwargs.sf_loss_coeff=0.5 \
    algo.kwargs.reward_loss_coeff=0.5 \
    algo.kwargs.entropy_loss_coeff=0.01 \
    algo.kwargs.clip_grad_norm=1.0 \

# (4) Copy things over to scratch?
# cp $EXP_LOG_PATH /network/tmp1/chenant/tmp/
