#!/bin/bash

# ============================================================================
# Script submitted to sbatch to run a hydra parameter sweep on the cluster
#
# Author: Anthony G. Chen
# ============================================================================

# Format for where to put the parameter sweep outputs
sweep_parent_dir='/network/tmp1/chenant/ant/sr_trace/${now:%Y-%m-%d}/${now:%H-%M-%S}'

# ===========================
# Experimental set-up

# (1.1) Load packages
module load python/3.7
module load python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.0

# (1.2) Load environment
source $HOME/venvs/rlpyt/bin/activate

# (2) no data to copy

# (3) launch job with parameter sweeps
python -u train_incremental.py --multirun \
    hydra.run.dir=$sweep_parent_dir \
    hydra.sweep.dir=$sweep_parent_dir \
    runner.n_steps=5e6 \
    runner.kwargs.log_interval_episodes=50 \
    runner.kwargs.store_checkpoint=True \
    runner.kwargs.checkpoint_freq=5000 \
    env.kwargs.env_name='asterix' \
    algo=lsf_ac_lambda \
    algo.kwargs.lr_alpha=0.00048828125 \
    algo.kwargs.trace_lambda=0.0 \
    algo.kwargs.sf_lambda=0.3,0.8 \
    model=lsf_ac_network \
    model.kwargs.fc_sizes=128 \
    model.kwargs.sf_hidden_sizes=null \
    model.kwargs.detach_sf_grad=True \
    training.seed=9,12,15 \


# (4) Copy things over to scratch?
# cp $EXP_LOG_PATH /network/tmp1/chenant/tmp/
